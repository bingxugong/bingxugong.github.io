# 学习APIwrapper笔记

### 关于token
#### 保存token
https://chat.openai.com/api/auth/session

#### 设置环境变量
```bash
sudo gedit ~/.bashrc
```
添加 
```commandline
export TOKEN="..."
```
生效
```commandline
source ~/.bashrc
```
查看
```commandline
echo $TOKEN
```

## 主体代码
```python
#!/usr/bin/env python
import asyncio
import base64
import json
import typing
from uuid import uuid4

import httpx
import websockets
import logging
from websockets.exceptions import ConnectionClosedError, ConnectionClosedOK


class MessageDeserializer(object):
    def __init__(self, data: str) -> None:
        self.data = data.lstrip("data: ").strip()

    def __str__(self) -> str:
        try:
            js = json.loads(self.data)
            return js['message']['content']['parts'][0]
        except json.decoder.JSONDecodeError:
            return ""
        except KeyError:
            logging.error(f"Error: {self.data}")
            return ""


class AsyncRapper(object):
    def __init__(self,
                 access_token: str,
                 model: str = "text-davinci-002-render-sha") -> None:
        """ API (w)rapper for OpenAI's ChatGPT.
        Args:
            access_token (str): ChatGPT access token, acquired from https://chat.openai.com/api/auth/session
            model (str): model name, options include:
                - "text-davinci-002-render-sha", default model for ChatGPT-3.5
                - "GPT-4", GPT-4 model
        """
        self.access_token = access_token
        self.model = model

    async def _stream_from_wss(self, chunk: str) -> typing.AsyncGenerator[str, None]:
        url = json.loads(chunk)['wss_url']
        async with websockets.connect(url) as websocket:
            while True:
                try:
                    response = await websocket.recv()
                    body = json.loads(response)["body"]
                    body = base64.b64decode(body).decode('utf-8')
                    if 'DONE' in body:
                        break
                    yield body
                except ConnectionClosedOK:
                    break
                except ConnectionClosedError:
                    break

    async def stream(self,
                     text: str) -> typing.AsyncGenerator[str, None]:
        body = {
            "action": "next",
            "arkose_token": "null",
            "conversation_mode": {"kind": "primary_assistant"},
            "force_paragen": False,
            "force_rate_limit": False,
            "history_and_training_disabled": True,
            "messages": [{
                "metadata": {},
                "author": {
                    "role": "user"
                },
                "content": {
                    "content_type": "text",
                    "parts": [text]
                }
            }],
            "model": self.model,
            "parent_message_id": str(uuid4()),
            "timezone_offset_min": -330,
            "stream": True
        }

        async with httpx.AsyncClient() as client:
            async with client.stream(
                'POST',
                url="https://chat.openai.com/backend-api/conversation",
                headers={
                    "accept": "text/event-stream",
                    "accept-language": "en-US",
                    "authorization": f"Bearer {self.access_token}",
                    "content-type": "application/json",
                    "sec-fetch-dest": "empty",
                    "sec-fetch-mode": "cors",
                    "sec-fetch-site": "same-origin",
                    "Referer": "https://chat.openai.com/",
                    "Referrer-Policy": "strict-origin-when-cross-origin",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36"
                },
                    data=json.dumps(body)) as response:
                async for chunk in response.aiter_text():
                    chunk = chunk.lstrip("data: ").strip()
                    if "wss_url" in chunk:
                        async for x in self._stream_from_wss(chunk):
                            yield str(MessageDeserializer(x))
                    else:
                        yield str(MessageDeserializer(chunk))

    async def __call__(self, text: str) -> str:
        prev = ""
        async for x in self.stream(text):
            print(x.replace(prev, ""), end="", flush=True)
            prev = max(prev, x, key=len)
        return prev


class Rapper(object):
    def __init__(self,
                 access_token: str,
                 model: str = "text-davinci-002-render-sha") -> None:
        self._proxy = AsyncRapper(access_token, model)

    def __call__(self, text: str) -> str:
        return asyncio.run(self._proxy(text))

```

### 对于self._proxy部分
对的，Rapper类中的__call__方法实际上是通过asyncio.run来运行AsyncRapper的方法。在Rapper类的初始化中，会创建一个AsyncRapper的实例，并将其赋值给self._proxy属性。然后在__call__方法中，通过asyncio.run(self._proxy(text))来调用AsyncRapper实例的方法，实现了对API的调用和异步执行。

这种设计模式是为了在同步代码中使用异步功能，通过asyncio.run在同步上下文中运行异步函数，从而实现了异步调用的效果。

### 对于proxy报错
我的test1.py代码
```python
import chatrapper
token = 'eyJhb1wc...'
rapper = chatrapper.Rapper(access_token=token)
text = "nihao"
a = rapper(text)
print(a)
```

报错内容
```commandline

/home/gbx/anaconda3/envs/py38_APIwrapper/bin/python /home/gbx/Downloads/chatrapper-master/tests/test1.py 
Traceback (most recent call last):
  File "/home/gbx/Downloads/chatrapper-master/tests/test1.py", line 5, in <module>
    a = rapper(text)
  File "/home/gbx/Downloads/chatrapper-master/chatrapper/__init__.py", line 124, in __call__
    return asyncio.run(self._proxy(text))
  File "/home/gbx/anaconda3/envs/py38_APIwrapper/lib/python3.8/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/home/gbx/anaconda3/envs/py38_APIwrapper/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/home/gbx/Downloads/chatrapper-master/chatrapper/__init__.py", line 111, in __call__
    async for x in self.stream(text):
  File "/home/gbx/Downloads/chatrapper-master/chatrapper/__init__.py", line 84, in stream
    async with httpx.AsyncClient() as client:
  File "/home/gbx/anaconda3/envs/py38_APIwrapper/lib/python3.8/site-packages/httpx/_client.py", line 1429, in __init__
    proxy_map = self._get_proxy_map(proxies or proxy, allow_env_proxies)
  File "/home/gbx/anaconda3/envs/py38_APIwrapper/lib/python3.8/site-packages/httpx/_client.py", line 217, in _get_proxy_map
    return {
  File "/home/gbx/anaconda3/envs/py38_APIwrapper/lib/python3.8/site-packages/httpx/_client.py", line 218, in <dictcomp>
    key: None if url is None else Proxy(url=url)
  File "/home/gbx/anaconda3/envs/py38_APIwrapper/lib/python3.8/site-packages/httpx/_config.py", line 336, in __init__
    raise ValueError(f"Unknown scheme for proxy URL {url!r}")
ValueError: Unknown scheme for proxy URL URL('socks://127.0.0.1:7890/')

```
GPT回复

这个错误是因为在创建httpx.AsyncClient时，使用了一个不支持的代理URL。具体来说，代理URL中的scheme是socks://，而httpx目前不支持SOCKS代理。为了解决这个问题，你可以考虑以下几种方法：

更改代理类型：将代理类型改为http://或https://类型的代理，这样httpx就能够正确地处理代理请求。例如，将代理URL修改为http://127.0.0.1:7890/或https://127.0.0.1:7890/。

使用支持SOCKS代理的库：如果你需要使用SOCKS代理，可以考虑使用支持SOCKS代理的库，比如aiohttp，它可以处理SOCKS代理。

禁用代理：如果你不需要代理或者无法更改代理类型，可以考虑在httpx.AsyncClient初始化时不传入代理参数，这样就会使用系统默认的网络设置而不使用代理。

根据你的实际需求和网络环境，选择适合的方法来解决这个问题。

如果你想要修改代理URL为http://127.0.0.1:7890/或https://127.0.0.1:7890/，可以按照以下步骤进行修改：
在你的代码中，找到使用httpx.AsyncClient初始化的地方，通常是在stream方法中的async with httpx.AsyncClient()这行代码，然后将代理URL修改为http://127.0.0.1:7890/或https://127.0.0.1:7890/，具体取决于你的代理类型（HTTP还是HTTPS）。

示例代码修改后的样子可能是这样的：
```python
async def stream(self, text: str) -> typing.AsyncGenerator[str, None]:
    body = {
        "action": "next",
        "arkose_token": "null",
        "conversation_mode": {"kind": "primary_assistant"},
        "force_paragen": False,
        "force_rate_limit": False,
        "history_and_training_disabled": True,
        "messages": [{
            "metadata": {},
            "author": {
                "role": "user"
            },
            "content": {
                "content_type": "text",
                "parts": [text]
            }
        }],
        "model": self.model,
        "parent_message_id": str(uuid4()),
        "timezone_offset_min": -330,
        "stream": True
    }

    proxies = {"https://127.0.0.1:7890/"} if your_proxy_is_https else {"http://127.0.0.1:7890/"}
    async with httpx.AsyncClient(proxies=proxies) as client:
        async with client.stream(
            'POST',
            url="https://chat.openai.com/backend-api/conversation",
            headers={
                "accept": "text/event-stream",
                "accept-language": "en-US",
                "authorization": f"Bearer {self.access_token}",
                "content-type": "application/json",
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "Referer": "https://chat.openai.com/",
                "Referrer-Policy": "strict-origin-when-cross-origin",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36"
            },
            data=json.dumps(body)) as response:
            async for chunk in response.aiter_text():
                chunk = chunk.lstrip("data: ").strip()
                if "wss_url" in chunk:
                    async for x in self._stream_from_wss(chunk):
                        yield str(MessageDeserializer(x))
                else:
                    yield str(MessageDeserializer(chunk))
```
。。。。
鉴定为没吊用

草好难啊
先放一边我先学一下基础的请求响应吧
不然这个都不知道怎么改动
